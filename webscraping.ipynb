{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEB-SCRAPPING\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import html\n",
    "import json\n",
    "import re\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"politics\",\"world\",\"us\",\"business\",\"health\",\"science\",\n",
    "    \"entertainment\",\"tech\",\"sports\",\"opinion\",\"media\",\"education\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://www.foxnews.com/category/{category}?page={page}\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def extract_author(soup):\n",
    "    for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        if not tag.string:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(tag.string)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        items = []\n",
    "        if isinstance(data, dict):\n",
    "            items.append(data)\n",
    "            if \"@graph\" in data:\n",
    "                items.extend(data[\"@graph\"])\n",
    "        elif isinstance(data, list):\n",
    "            items.extend(data)\n",
    "\n",
    "        for item in items:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            author = item.get(\"author\") or item.get(\"creator\")\n",
    "            if not author:\n",
    "                continue\n",
    "            if isinstance(author, dict) and \"name\" in author:\n",
    "                return author[\"name\"]\n",
    "            if isinstance(author, list) and len(author) > 0:\n",
    "                first = author[0]\n",
    "                if isinstance(first, dict) and \"name\" in first:\n",
    "                    return first[\"name\"]\n",
    "            if isinstance(author, str):\n",
    "                return author\n",
    "\n",
    "    tag = soup.find(\"meta\", {\"name\": \"author\"}) or soup.find(\"meta\", {\"property\": \"article:author\"})\n",
    "    if tag and tag.get(\"content\"):\n",
    "        return tag.get(\"content\")\n",
    "\n",
    "    byline = soup.find(string=re.compile(r\"^\\s*By\\s+\", re.I))\n",
    "    if byline:\n",
    "        return byline.strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def clean_author(name):\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    name = html.unescape(name).strip()\n",
    "    name = re.sub(r\"^\\s*By\\s*[:\\--]?\\s*\", \"\", name, flags=re.I)\n",
    "    name = re.sub(r\"(?:FOX NEWS.*$)\", \"\", name, flags=re.I)\n",
    "    name = re.split(r\",| and | & \", name)[0].strip()\n",
    "    m = re.match(r\"^[A-Z][\\w'’`.-]+(?:\\s+[A-Z][\\w'’`.-]+){0,4}$\", name)\n",
    "    return m.group(0) if m else name\n",
    "\n",
    "def extract_body_json(soup):\n",
    "    for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(tag.string)\n",
    "        except:\n",
    "            continue\n",
    "        if isinstance(data, dict):\n",
    "            if \"articleBody\" in data:\n",
    "                return data[\"articleBody\"]\n",
    "            if \"@graph\" in data:\n",
    "                for x in data[\"@graph\"]:\n",
    "                    if isinstance(x, dict) and \"articleBody\" in x:\n",
    "                        return x[\"articleBody\"]\n",
    "    return \"\"\n",
    "\n",
    "def extract_body_html(soup):\n",
    "    div = soup.find(\"div\", class_=\"article-body\")\n",
    "    if not div:\n",
    "        return \"\"\n",
    "    return \" \".join(p.get_text(\" \", strip=True) for p in div.find_all(\"p\")).strip()\n",
    "\n",
    "def scrape_article(url, category):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"\"\n",
    "        date = soup.find(\"time\").get(\"datetime\", \"\") if soup.find(\"time\") else \"\"\n",
    "        author = clean_author(extract_author(soup))\n",
    "\n",
    "        body = extract_body_json(soup) or extract_body_html(soup)\n",
    "        body = html.unescape(body)\n",
    "\n",
    "        if len(body) < 200:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"category\": category,\n",
    "            \"articleBody\": body\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def scrape_page(category, page):\n",
    "    try:\n",
    "        url = BASE_URL.format(category=category, page=page)\n",
    "        r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        links = {\n",
    "            \"https://www.foxnews.com\" + a[\"href\"]\n",
    "            for a in soup.find_all(\"a\", href=True)\n",
    "            if a[\"href\"].startswith(f\"/{category}/\")\n",
    "        }\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def scrape_articles_all(categories, pages=20):\n",
    "    total = []\n",
    "    for cat in categories:\n",
    "        for page in range(1, pages + 1):\n",
    "            for url in scrape_page(cat, page):\n",
    "                data = scrape_article(url, cat)\n",
    "                if data:\n",
    "                    total.append(data)\n",
    "                if len(total) >= 1000:\n",
    "                    pd.DataFrame(total).to_csv(\"foxnews_articles.csv\", index=False)\n",
    "                    return\n",
    "    pd.DataFrame(total).to_csv(\"foxnews.csv\", index=False)\n",
    "\n",
    "scrape_articles_all(CATEGORIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee96e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "\n",
    "def fix_encoding(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv(\"foxnews_articles.csv\")  \n",
    "\n",
    "for col in [\"title\", \"author\", \"articleBody\"]:\n",
    "    df[col] = df[col].apply(fix_encoding)\n",
    "\n",
    "df.to_csv(\"foxnews_articless.csv\", index=False) \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
