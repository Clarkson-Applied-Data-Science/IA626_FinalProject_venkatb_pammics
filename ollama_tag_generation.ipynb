{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ollama\n",
    "import json\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MODEL = \"mistral:instruct\"\n",
    "LIMIT = 30\n",
    "BATCH_SIZE = 2\n",
    "THREADS = 4\n",
    "MAX_TAGS = 10\n",
    "\n",
    "def extract_json_array(raw):\n",
    "    m = re.search(r\"\\[.*\\]\", raw, re.DOTALL)\n",
    "    return m.group(0) if m else \"[]\"\n",
    "\n",
    "def build_batch_prompt(batch):\n",
    "    formatted_articles = \"\\n\\n\".join([f\"ARTICLE {i}:\\n{text}\" for i, text in batch])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a semantic information extractor.\n",
    "\n",
    "Your job is to create *ONE unified list of tags* for each article.\n",
    "\n",
    "Each tag must be a *high-level conceptual theme* derived from:\n",
    "- the meaning of the article,\n",
    "- the emotions implicitly expressed,\n",
    "- the type of locations involved.\n",
    "\n",
    "STRICT RULES:\n",
    "- NEVER output labels like \"Emotion: X\" or \"Location: X\".\n",
    "- NEVER output standalone emotion words.\n",
    "- NEVER output standalone location category words.\n",
    "- ALL emotions must be embedded into conceptual tags.\n",
    "- ALL locations must be converted into actual location.\n",
    "- DO NOT use any exact words from the article.\n",
    "- DO NOT output city names, country names, or real entities seperately\n",
    "- DO NOT output categories like \"emotion\", \"location\", \"state\", \"country\".\n",
    "\n",
    "Example good tags:\n",
    "[\n",
    "    \"news\",\"opinion\",\"analysis\",\"feature\",\"interview\",\"guide\",\"how-to\",\"tips\",\"trends\",\n",
    "    \"United States\",\"United Kingdom\",\"Canada\",\"Australia\",\"India\",\"South Africa\",\n",
    "    \"Germany\",\"France\",\"Brazil\",\"Japan\",\"New York\",\"London\",\"Toronto\",\"Sydney\",\n",
    "    \"Mumbai\",\"Cape Town\",\"Berlin\",\"Paris\",\"SÃ£o Paulo\",\"Tokyo\",\"North America\",\n",
    "    \"Europe\",\"Southeast Asia\",\"Middle East\",\"Sub-Saharan Africa\",\"Latin America\",\n",
    "    \"Silicon Valley\",\"The Caribbean\",\"The Arctic\",\"West Coast\",\"Midwest\",\n",
    "    \"inspirational\",\"emotional\",\"uplifting\",\"hopeful\",\"tense\",\"dramatic\",\n",
    "    \"heartbreaking\",\"motivating\",\"empowering\",\"humorous\",\"reflective\",\n",
    "    \"optimistic\",\"appreciative\",\"nostalgic\",\"shocking\",\"urgent\",\"Google\",\n",
    "    \"Apple\",\"Microsoft\",\"Meta\",\"Amazon\",\"Tesla\",\"Samsung\",\"OpenAI\",\"Red Cross\",\n",
    "    \"UNICEF\",\"WWF\",\"Amnesty International\",\"Doctors Without Borders\",\"NASA\",\n",
    "    \"NHS\",\"CDC\",\"EPA\",\"European Union\",\"United Nations\",\"Harvard University\",\n",
    "    \"MIT\",\"Oxford University\",\"Stanford University\"\n",
    "]\n",
    "\n",
    "OUTPUT ONLY JSON:\n",
    "\n",
    "[\n",
    "{{\n",
    "    \"article_id\": 0,\n",
    "    \"tags\": []\n",
    "}},\n",
    "{{\n",
    "    \"article_id\": 1,\n",
    "    \"tags\": []\n",
    "}}\n",
    "]\n",
    "\n",
    "ARTICLES:\n",
    "{formatted_articles}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def process_batch(batch):\n",
    "    prompt = build_batch_prompt(batch)\n",
    "    resp = ollama.generate(model=MODEL, prompt=prompt, stream=False)\n",
    "    raw = resp.get(\"response\", \"\")\n",
    "    js_array = extract_json_array(raw)\n",
    "\n",
    "    try:\n",
    "        data = json.loads(js_array)\n",
    "    except:\n",
    "        data = []\n",
    "\n",
    "    fixed = {aid: {\"article_id\": aid, \"tags\": []} for aid, _ in batch}\n",
    "\n",
    "    for item in data:\n",
    "        aid = item.get(\"article_id\")\n",
    "        if aid in fixed:\n",
    "            tags = item.get(\"tags\", [])\n",
    "            cleaned = []\n",
    "            for t in tags:\n",
    "                if isinstance(t, str) and t.strip() and \"placeholder\" not in t.lower():\n",
    "                    cleaned.append(t.strip())\n",
    "            cleaned = cleaned[:MAX_TAGS]\n",
    "            fixed[aid][\"tags\"] = cleaned\n",
    "\n",
    "    return list(fixed.values())\n",
    "\n",
    "def pipeline(csv_path):\n",
    "    contents = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for idx, row in enumerate(reader):\n",
    "            if idx >= LIMIT:\n",
    "                break\n",
    "            contents.append((idx, row[\"articleBody\"]))\n",
    "    batches = [contents[i:i+BATCH_SIZE] for i in range(0, len(contents), BATCH_SIZE)]\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as ex:\n",
    "        futures = {ex.submit(process_batch, batch): batch for batch in batches}\n",
    "        for future in as_completed(futures):\n",
    "            for item in future.result():\n",
    "                results[item[\"article_id\"]] = item\n",
    "    return [results[i] for i in range(LIMIT)]\n",
    "\n",
    "output = pipeline(\"foxnews_articless.csv\")\n",
    "\n",
    "with open(\"semantic_tags.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(\"Saved as semantic_tags.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
